{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cEH6MHGH0c3",
        "outputId": "f7d65bdd-794e-41f4-e7c9-378c704a5e65"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import argparse\n",
        "from random import random\n",
        "\n",
        "from torch import optim\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Function\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence"
      ],
      "metadata": {
        "id": "Jj9jpF54iery"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(au_mfcc_path):\n",
        "    data = []\n",
        "    labels = []\n",
        "    with open(au_mfcc_path, 'rb') as f:\n",
        "        au_mfcc = pickle.load(f)\n",
        "\n",
        "    print(len(au_mfcc))\n",
        "\n",
        "    for key in au_mfcc:\n",
        "        emotion = key.split('-')[2]\n",
        "        emotion = int(emotion)-1\n",
        "        labels.append(emotion)\n",
        "        data.append(au_mfcc[key])\n",
        "\n",
        "    data=np.array(data)\n",
        "    labels = np.array(labels)\n",
        "    labels = labels.reshape(labels.shape+(1,))\n",
        "\n",
        "    data = np.hstack((data, labels))\n",
        "    fdata = shuffle(data)\n",
        "\n",
        "    data = fdata[:, :-1]\n",
        "    labels = fdata[:, -1].astype(int)\n",
        "\n",
        "    return data, labels"
      ],
      "metadata": {
        "id": "XP8UGMq1idND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "_x7tP9HJO6ZJ"
      },
      "outputs": [],
      "source": [
        "class MMF_Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MMF_Model, self).__init__()\n",
        "\n",
        "        rnn = nn.LSTM\n",
        "\n",
        "        self.au_rnn1 = rnn(35, 16, bidirectional=True)\n",
        "        self.au_rnn2 = rnn(2*16, 16, bidirectional=True)\n",
        "\n",
        "        self.mfccs_rnn1 = rnn(259, 16, bidirectional=True)\n",
        "        self.mfccs_rnn2 = rnn(2*16, 16, bidirectional=True)\n",
        "\n",
        "        self.fusion_layer = nn.Linear(in_features=128, out_features=8)\n",
        "\n",
        "    def extract_au(self, au, lengths):\n",
        "        packed_sequence = pack_padded_sequence(au, lengths)\n",
        "        packed_h1, (final_h1, _) = self.au_rnn1(packed_sequence)\n",
        "        padded_h1, _ = pad_packed_sequence(packed_h1)\n",
        "        packed_normed_h1 = pack_padded_sequence(padded_h1, lengths)\n",
        "        _, (final_h2, _) = self.au_rnn2(packed_normed_h1)\n",
        "        extracted_au = torch.cat((final_h1, final_h2), dim=2).permute(1,0,2).contiguous().view(batch_size,-1)\n",
        "\n",
        "        return extracted_au\n",
        "\n",
        "    def extract_mfccs(self, mfccs, lengths):\n",
        "\n",
        "        packed_sequence = pack_padded_sequence(mfccs, lengths)\n",
        "        packed_h1, (final_h1, _) = self.mfccs_rnn1(packed_sequence)\n",
        "        padded_h1, _ = pad_packed_sequence(packed_h1)\n",
        "        packed_normed_h1 = pack_padded_sequence(padded_h1, lengths)\n",
        "        _, (final_h2, _) = self.mfccs_rnn2(packed_normed_h1)\n",
        "        extracted_mfccs = torch.cat((final_h1, final_h2), dim=2).permute(1,0,2).contiguous().view(batch_size,-1)\n",
        "\n",
        "        return extracted_mfccs\n",
        "\n",
        "    def forward(self, au, mfccs, lengths):\n",
        "        batch_size = 60\n",
        "\n",
        "        extracted_au = self.extract_au(au, lengths)\n",
        "        extracted_mfccs = self.extract_mfccs(mfccs, lengths)\n",
        "\n",
        "        au_mfccs_fusion = torch.cat((extracted_au, extracted_mfccs), dim=1)\n",
        "\n",
        "        final_output = self.fusion_layer(au_mfccs_fusion)\n",
        "        return final_output\n",
        "\n",
        "def eval(data, labels, mode=None, to_print=False):\n",
        "    assert(mode is not None)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    y_true, y_pred = [], []\n",
        "    eval_loss, eval_loss_diff = [], []\n",
        "\n",
        "    if mode == \"test\":\n",
        "        if to_print:\n",
        "            model.load_state_dict(torch.load(\n",
        "                f'/content/drive/MyDrive/multimodal-fusion/model.ckpt'))\n",
        "\n",
        "    corr=0\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(data), 60):\n",
        "            model.zero_grad()\n",
        "            # v, a, y, l = batch\n",
        "            d=data[i:i+60]\n",
        "            l=labels[i:i+60]\n",
        "            d=np.expand_dims(d,axis=0)\n",
        "            au=torch.from_numpy(d[:, :, :35]).float()\n",
        "            mfccs=torch.from_numpy(d[:, :, 35:]).float()\n",
        "            y=torch.from_numpy(l).float()\n",
        "\n",
        "            lengths = torch.LongTensor([au.shape[0]]*au.size(1))\n",
        "\n",
        "            au = au.cuda()\n",
        "            mfccs = mfccs.cuda()\n",
        "            y = y.cuda()\n",
        "\n",
        "            output = model(au, mfccs, lengths)\n",
        "\n",
        "            loss =  criterion(output, y)\n",
        "\n",
        "            eval_loss.append(loss.item())\n",
        "            preds=output.detach().cpu().numpy()\n",
        "            y_trues=y.detach().cpu().numpy()\n",
        "\n",
        "            for j in range(len(preds)):\n",
        "                pred=np.argmax(preds[j])\n",
        "                y_true=np.argmax(y_trues[j])\n",
        "                if pred==y_true:\n",
        "                    corr+=1\n",
        "\n",
        "    eval_loss = np.mean(eval_loss)\n",
        "\n",
        "    accuracy = corr/(1.0*len(labels))\n",
        "\n",
        "    return eval_loss, accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    device = torch.cuda.is_available()\n",
        "\n",
        "    data_path = r'/content/drive/MyDrive/multimodal-fusion/au_mfcc.pkl'\n",
        "\n",
        "    data, labels=preprocess(data_path)\n",
        "    print('u:', np.unique(labels.astype(int)).size)\n",
        "    new_labels= np.zeros((labels.shape[0], np.unique(labels.astype(int)).size))\n",
        "\n",
        "    for i in range(len(labels)):\n",
        "        new_labels[i, labels[i]]=1\n",
        "\n",
        "    labels=new_labels\n",
        "\n",
        "    test_data=data[-181:-1]\n",
        "    test_labels=labels[-181:-1]\n",
        "    data=data[:-180]\n",
        "    labels=labels[:-180]\n",
        "\n",
        "    train_data=data[:1020]\n",
        "    train_labels=labels[:1020]\n",
        "\n",
        "    dev_data=data[1020:]\n",
        "    dev_labels=labels[1020:]\n",
        "\n",
        "    model = MMF_Model()\n",
        "\n",
        "    model.cuda()\n",
        "\n",
        "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    batch_size=60\n",
        "    n_total=len(train_data)\n",
        "    best_loss=float('inf')\n",
        "    for e in range(50):\n",
        "        model.train()\n",
        "        total_loss=0\n",
        "        cnt=0\n",
        "        print(f\"=====Epoch{e+1}======\")\n",
        "        for i in range(0, len(train_data), batch_size):\n",
        "            data=train_data[i:i+60]\n",
        "            label=train_labels[i:i+60]\n",
        "\n",
        "            model.zero_grad()\n",
        "            # v, a, y, l = batch\n",
        "            data=np.expand_dims(data,axis=0)\n",
        "            au=torch.from_numpy(data[:, :, :35]).float()\n",
        "            mfccs=torch.from_numpy(data[:, :, 35:]).float()\n",
        "            y=torch.from_numpy(label).float()\n",
        "\n",
        "            au = au.cuda()\n",
        "            mfccs = mfccs.cuda()\n",
        "\n",
        "            y = y.cuda()\n",
        "\n",
        "            lengths = torch.LongTensor([au.shape[0]]*au.size(1))\n",
        "            fused_features = model(au, mfccs, lengths)\n",
        "\n",
        "            loss = criterion(fused_features, y)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "        train_loss, train_acc = eval(train_data, train_labels, mode=\"train\")\n",
        "        print('train_loss: {:.3f}, train_acc: {:.2f}%'.format(train_loss, 100*train_acc))\n",
        "\n",
        "        valid_loss, valid_acc = eval(dev_data, dev_labels, mode=\"dev\")\n",
        "        print('valid_loss: {:.3f}, valid_acc: {:.2f}%'.format(valid_loss, 100*valid_acc))\n",
        "\n",
        "        if valid_loss < best_loss:\n",
        "            best_loss = valid_loss\n",
        "            torch.save(model.state_dict(), '/content/drive/MyDrive/multimodal-fusion/model.ckpt')\n",
        "            torch.save(optimizer.state_dict(), '/content/drive/MyDrive/multimodal-fusion/optim_best.std')\n",
        "        else:\n",
        "            model.load_state_dict(torch.load('/content/drive/MyDrive/multimodal-fusion/model.ckpt'))\n",
        "            optimizer.load_state_dict(torch.load('/content/drive/MyDrive/multimodal-fusion/optim_best.std'))\n",
        "\n",
        "    test_loss, test_acc=eval(test_data, test_labels, mode=\"test\", to_print=True)\n",
        "    print('test_loss: {:.3f} test_acc: {:.2f}%'.format(test_loss, 100*test_acc))"
      ],
      "metadata": {
        "id": "m2cz6zB2I42R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33c81438-bcaa-493d-dbf5-4cad3503f89c"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1440\n",
            "u: 8\n",
            "=====Epoch1======\n",
            "train_loss: 2.030, train_acc: 26.47%\n",
            "valid_loss: 2.044, valid_acc: 21.25%\n",
            "=====Epoch2======\n",
            "train_loss: 1.969, train_acc: 39.51%\n",
            "valid_loss: 1.995, valid_acc: 28.33%\n",
            "=====Epoch3======\n",
            "train_loss: 1.877, train_acc: 37.65%\n",
            "valid_loss: 1.911, valid_acc: 29.17%\n",
            "=====Epoch4======\n",
            "train_loss: 1.734, train_acc: 45.00%\n",
            "valid_loss: 1.775, valid_acc: 35.83%\n",
            "=====Epoch5======\n",
            "train_loss: 1.580, train_acc: 44.51%\n",
            "valid_loss: 1.628, valid_acc: 39.17%\n",
            "=====Epoch6======\n",
            "train_loss: 1.450, train_acc: 50.49%\n",
            "valid_loss: 1.502, valid_acc: 45.00%\n",
            "=====Epoch7======\n",
            "train_loss: 1.354, train_acc: 53.33%\n",
            "valid_loss: 1.416, valid_acc: 48.75%\n",
            "=====Epoch8======\n",
            "train_loss: 1.276, train_acc: 56.27%\n",
            "valid_loss: 1.338, valid_acc: 52.92%\n",
            "=====Epoch9======\n",
            "train_loss: 1.216, train_acc: 58.04%\n",
            "valid_loss: 1.279, valid_acc: 55.42%\n",
            "=====Epoch10======\n",
            "train_loss: 1.157, train_acc: 59.71%\n",
            "valid_loss: 1.225, valid_acc: 54.17%\n",
            "=====Epoch11======\n",
            "train_loss: 1.105, train_acc: 61.96%\n",
            "valid_loss: 1.175, valid_acc: 58.33%\n",
            "=====Epoch12======\n",
            "train_loss: 1.057, train_acc: 63.43%\n",
            "valid_loss: 1.124, valid_acc: 59.58%\n",
            "=====Epoch13======\n",
            "train_loss: 1.023, train_acc: 64.80%\n",
            "valid_loss: 1.092, valid_acc: 60.00%\n",
            "=====Epoch14======\n",
            "train_loss: 0.987, train_acc: 65.78%\n",
            "valid_loss: 1.057, valid_acc: 60.83%\n",
            "=====Epoch15======\n",
            "train_loss: 0.945, train_acc: 67.06%\n",
            "valid_loss: 1.012, valid_acc: 64.58%\n",
            "=====Epoch16======\n",
            "train_loss: 0.907, train_acc: 68.04%\n",
            "valid_loss: 0.974, valid_acc: 65.42%\n",
            "=====Epoch17======\n",
            "train_loss: 0.885, train_acc: 68.24%\n",
            "valid_loss: 0.960, valid_acc: 63.33%\n",
            "=====Epoch18======\n",
            "train_loss: 0.861, train_acc: 68.92%\n",
            "valid_loss: 0.940, valid_acc: 63.75%\n",
            "=====Epoch19======\n",
            "train_loss: 0.842, train_acc: 69.61%\n",
            "valid_loss: 0.914, valid_acc: 67.08%\n",
            "=====Epoch20======\n",
            "train_loss: 0.816, train_acc: 70.78%\n",
            "valid_loss: 0.901, valid_acc: 65.83%\n",
            "=====Epoch21======\n",
            "train_loss: 0.791, train_acc: 71.57%\n",
            "valid_loss: 0.875, valid_acc: 67.50%\n",
            "=====Epoch22======\n",
            "train_loss: 0.781, train_acc: 71.08%\n",
            "valid_loss: 0.871, valid_acc: 68.75%\n",
            "=====Epoch23======\n",
            "train_loss: 0.743, train_acc: 72.35%\n",
            "valid_loss: 0.833, valid_acc: 66.25%\n",
            "=====Epoch24======\n",
            "train_loss: 0.728, train_acc: 74.22%\n",
            "valid_loss: 0.816, valid_acc: 67.92%\n",
            "=====Epoch25======\n",
            "train_loss: 0.716, train_acc: 73.73%\n",
            "valid_loss: 0.811, valid_acc: 69.58%\n",
            "=====Epoch26======\n",
            "train_loss: 0.703, train_acc: 73.92%\n",
            "valid_loss: 0.791, valid_acc: 70.00%\n",
            "=====Epoch27======\n",
            "train_loss: 0.690, train_acc: 74.31%\n",
            "valid_loss: 0.784, valid_acc: 70.42%\n",
            "=====Epoch28======\n",
            "train_loss: 0.658, train_acc: 75.10%\n",
            "valid_loss: 0.762, valid_acc: 70.83%\n",
            "=====Epoch29======\n",
            "train_loss: 0.646, train_acc: 76.57%\n",
            "valid_loss: 0.748, valid_acc: 74.58%\n",
            "=====Epoch30======\n",
            "train_loss: 0.637, train_acc: 75.78%\n",
            "valid_loss: 0.743, valid_acc: 73.75%\n",
            "=====Epoch31======\n",
            "train_loss: 0.613, train_acc: 77.65%\n",
            "valid_loss: 0.729, valid_acc: 72.50%\n",
            "=====Epoch32======\n",
            "train_loss: 0.602, train_acc: 77.84%\n",
            "valid_loss: 0.719, valid_acc: 72.92%\n",
            "=====Epoch33======\n",
            "train_loss: 0.605, train_acc: 78.53%\n",
            "valid_loss: 0.699, valid_acc: 75.42%\n",
            "=====Epoch34======\n",
            "train_loss: 0.575, train_acc: 78.73%\n",
            "valid_loss: 0.694, valid_acc: 75.83%\n",
            "=====Epoch35======\n",
            "train_loss: 0.562, train_acc: 79.22%\n",
            "valid_loss: 0.688, valid_acc: 75.42%\n",
            "=====Epoch36======\n",
            "train_loss: 0.565, train_acc: 80.29%\n",
            "valid_loss: 0.694, valid_acc: 75.42%\n",
            "=====Epoch37======\n",
            "train_loss: 0.565, train_acc: 80.29%\n",
            "valid_loss: 0.694, valid_acc: 75.42%\n",
            "=====Epoch38======\n",
            "train_loss: 0.565, train_acc: 80.29%\n",
            "valid_loss: 0.694, valid_acc: 75.42%\n",
            "=====Epoch39======\n",
            "train_loss: 0.565, train_acc: 80.29%\n",
            "valid_loss: 0.694, valid_acc: 75.42%\n",
            "=====Epoch40======\n",
            "train_loss: 0.565, train_acc: 80.29%\n",
            "valid_loss: 0.694, valid_acc: 75.42%\n",
            "=====Epoch41======\n",
            "train_loss: 0.565, train_acc: 80.29%\n",
            "valid_loss: 0.694, valid_acc: 75.42%\n",
            "=====Epoch42======\n",
            "train_loss: 0.565, train_acc: 80.29%\n",
            "valid_loss: 0.694, valid_acc: 75.42%\n",
            "=====Epoch43======\n",
            "train_loss: 0.565, train_acc: 80.29%\n",
            "valid_loss: 0.694, valid_acc: 75.42%\n",
            "=====Epoch44======\n",
            "train_loss: 0.565, train_acc: 80.29%\n",
            "valid_loss: 0.694, valid_acc: 75.42%\n",
            "=====Epoch45======\n",
            "train_loss: 0.565, train_acc: 80.29%\n",
            "valid_loss: 0.694, valid_acc: 75.42%\n",
            "=====Epoch46======\n",
            "train_loss: 0.565, train_acc: 80.29%\n",
            "valid_loss: 0.694, valid_acc: 75.42%\n",
            "=====Epoch47======\n",
            "train_loss: 0.565, train_acc: 80.29%\n",
            "valid_loss: 0.694, valid_acc: 75.42%\n",
            "=====Epoch48======\n",
            "train_loss: 0.565, train_acc: 80.29%\n",
            "valid_loss: 0.694, valid_acc: 75.42%\n",
            "=====Epoch49======\n",
            "train_loss: 0.565, train_acc: 80.29%\n",
            "valid_loss: 0.694, valid_acc: 75.42%\n",
            "=====Epoch50======\n",
            "train_loss: 0.565, train_acc: 80.29%\n",
            "valid_loss: 0.694, valid_acc: 75.42%\n",
            "test_loss: 0.738 test_acc: 75.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pwpamMIBdryE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
